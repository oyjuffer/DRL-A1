{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - CATCH\n",
    "TODO:\n",
    "- Experience Replay Buffer - Done\n",
    "- Mini Batch Training\n",
    "- A Target Network\n",
    "- Reward Clipping\n",
    "- Loss Function Implementation\n",
    "- CNN\n",
    "\n",
    "Sources: \\\n",
    "https://github.com/yxu1168/Reinforcement-Learning-DQN-for-ATARI-s-Pong-Game---TensorFlow-2.0-Keras/blob/master/Reinforcement_Learning_CNN_DQN_for_ATARI%E2%80%99s_Pong_Game.ipynb\n",
    "\n",
    "https://github.com/PacktPublishing/Hands-on-Reinforcement-Learning-with-PyTorch/blob/master/Section%203/3.5%20DQN%20with%20Pong.ipynb\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatchEnv():\n",
    "    def __init__(self):\n",
    "        self.size = 21\n",
    "        self.image = np.zeros((self.size, self.size))\n",
    "        self.state = []\n",
    "        self.fps = 4\n",
    "        self.output_shape = (84, 84)\n",
    "\n",
    "    def reset_random(self):\n",
    "        self.image.fill(0)\n",
    "        self.pos = np.random.randint(2, self.size-2)\n",
    "        self.vx = np.random.randint(5) - 2\n",
    "        self.vy = 1\n",
    "        self.ballx, self.bally = np.random.randint(self.size), 4\n",
    "        self.image[self.bally, self.ballx] = 1\n",
    "        self.image[-5, self.pos - 2:self.pos + 3] = np.ones(5)\n",
    "\n",
    "        return self.step(2)[0]\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        def left():\n",
    "            if self.pos > 3:\n",
    "                self.pos -= 2\n",
    "        def right():\n",
    "            if self.pos < 17:\n",
    "                self.pos += 2\n",
    "        def noop():\n",
    "            pass\n",
    "        {0: left, 1: right, 2: noop}[action]()\n",
    "\n",
    "\n",
    "        self.image[self.bally, self.ballx] = 0\n",
    "        self.ballx += self.vx\n",
    "        self.bally += self.vy\n",
    "        if self.ballx > self.size - 1:\n",
    "            self.ballx -= 2 * (self.ballx - (self.size-1))\n",
    "            self.vx *= -1\n",
    "        elif self.ballx < 0:\n",
    "            self.ballx += 2 * (0 - self.ballx)\n",
    "            self.vx *= -1\n",
    "        self.image[self.bally, self.ballx] = 1\n",
    "\n",
    "        self.image[-5].fill(0)\n",
    "        self.image[-5, self.pos-2:self.pos+3] = np.ones(5)\n",
    "    \n",
    "        terminal = self.bally == self.size - 1 - 4\n",
    "        reward = int(self.pos - 2 <= self.ballx <= self.pos + 2) if terminal else 0\n",
    "\n",
    "        [self.state.append(resize(self.image, (84, 84))) for _ in range(self.fps - len(self.state) + 1)]\n",
    "        self.state = self.state[-self.fps:]\n",
    "\n",
    "        return np.transpose(self.state, [1, 2, 0]), reward, terminal\n",
    "\n",
    "    def get_num_actions(self):\n",
    "        return 3\n",
    "\n",
    "    def reset(self):\n",
    "        return self.reset_random()\n",
    "\n",
    "    def state_shape(self):\n",
    "        return (self.fps,) + self.output_shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size = 1e6):\n",
    "        self.buffer = []\n",
    "        self.max_size = max_size\n",
    "        self.overFlow = 0\n",
    "    \n",
    "    def add(self, data):\n",
    "        if len(self.buffer) == self.max_size:\n",
    "            self.buffer[self.overFlow] = data\n",
    "            self.overFlow = (self.overFlow + 1) % self.max_size\n",
    "        else:\n",
    "            self.buffer.append(data)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.randint(0, len(self.buffer), size=batch_size)\n",
    "        states, actions, rewards, states_next = [], [], [], []\n",
    "\n",
    "\n",
    "        for i in indices:\n",
    "            states, actions, rewards, states_next = self.buffer[i]\n",
    "\n",
    "        return states, actions, rewards, states_next"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_72 (Conv2D)          (None, 20, 20, 32)        8224      \n",
      "                                                                 \n",
      " conv2d_73 (Conv2D)          (None, 9, 9, 64)          32832     \n",
      "                                                                 \n",
      " conv2d_74 (Conv2D)          (None, 8, 8, 64)          16448     \n",
      "                                                                 \n",
      " flatten_24 (Flatten)        (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_72 (Dense)            (None, 64)                262208    \n",
      "                                                                 \n",
      " dense_73 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_74 (Dense)            (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 324,067\n",
      "Trainable params: 324,067\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (84, 84, 4)\n",
    "action_size = 3\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=8, strides=4, input_shape = input_shape, activation=\"relu\"))\n",
    "model.add(keras.layers.Conv2D(64, kernel_size=4, strides=2, activation=\"relu\"))\n",
    "model.add(keras.layers.Conv2D(64, kernel_size=2, strides=1, activation=\"relu\"))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(64, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(64, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(action_size))\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "mse_loss = keras.losses.mean_squared_error\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self):\n",
    "        self.action_size = 3\n",
    "        self.train_net = model\n",
    "        self.target_net = model\n",
    "        self.batch_size = 4\n",
    "\n",
    "        self.gamma = 0.95\n",
    "\n",
    "    # select an action based on epsilon greedy method\n",
    "    def action(self, state, epsilon):\n",
    "        \n",
    "        if np.random.rand() <= epsilon:\n",
    "            return random.randint(0, 2)\n",
    "        else:\n",
    "            return np.argmax(self.train_net.predict(state))\n",
    "\n",
    "    # train the model\n",
    "    def train(self, replay_buffer):\n",
    "\n",
    "        # sample from the experience replay buffer\n",
    "        states, actions, rewards, states_next = replay_buffer.sample(self.batch_size)\n",
    "\n",
    "        # train Q\n",
    "        train_q = self.train_net(states)\n",
    "\n",
    "        # target Q\n",
    "        target_q = rewards + self.gamma * tf.reduce_max(self.target_net(states_next), axis=-1)\n",
    "\n",
    "        # loss\n",
    "        loss = tf.reduce_mean(mse_loss(train_q, target_q))\n",
    "\n",
    "        # update weights of train_net\n",
    "        self.train_net.optimizer.minimize(loss, self.train_net.trainable_variables)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    # update the target network\n",
    "    def update():\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_environment():\n",
    "    env = CatchEnv()\n",
    "    agent = Agent()\n",
    "    replay_buffer = ReplayBuffer()\n",
    "\n",
    "    # hyperparameters\n",
    "    episodes = 10\n",
    "    epsilon = 0.1\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        env.reset()\n",
    "        state, reward, terminal = env.step(2)\n",
    "\n",
    "        while not terminal:\n",
    "            state = np.reshape(state, (1, 84, 84, 4))  \n",
    "            \n",
    "            action = agent.action(state, epsilon)\n",
    "            state_next, reward, terminal = env.step(action)\n",
    "\n",
    "            state_next = np.reshape(state_next, (1, 84, 84, 4)) \n",
    "\n",
    "            # add experience to replay buffer\n",
    "            replay_buffer.add((state, reward, action, state_next))\n",
    "\n",
    "            # train the agent\n",
    "            loss = agent.train(replay_buffer)\n",
    "\n",
    "\n",
    "            print(\"Loss: {}\".format(loss))\n",
    "            print(\"Reward obtained by the agent: {}\".format(reward))\n",
    "            state = np.squeeze(state)\n",
    "\n",
    "            # plt.matshow(state)\n",
    "            # plt.show()\n",
    "\n",
    "            state = state_next\n",
    "\n",
    "        print(\"End of the episode\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 54ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`tape` is required when a `Tensor` loss is passed. Received: loss=4.041056156158447, tape=None.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[175], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m run_environment()\n",
      "Cell \u001b[1;32mIn[174], line 26\u001b[0m, in \u001b[0;36mrun_environment\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m replay_buffer\u001b[39m.\u001b[39madd((state, reward, action, state_next))\n\u001b[0;32m     25\u001b[0m \u001b[39m# train the agent\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m loss \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mtrain(replay_buffer)\n\u001b[0;32m     29\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoss: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(loss))\n\u001b[0;32m     30\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mReward obtained by the agent: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(reward))\n",
      "Cell \u001b[1;32mIn[173], line 34\u001b[0m, in \u001b[0;36mAgent.train\u001b[1;34m(self, replay_buffer)\u001b[0m\n\u001b[0;32m     31\u001b[0m loss \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreduce_mean(mse_loss(train_q, target_q))\n\u001b[0;32m     33\u001b[0m \u001b[39m# update weights of train_net\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_net\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mminimize(loss, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_net\u001b[39m.\u001b[39;49mtrainable_variables)\n\u001b[0;32m     36\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\oyjuf\\Documents\\GitHub\\DRL-A1\\.venv\\lib\\site-packages\\keras\\optimizers\\optimizer.py:542\u001b[0m, in \u001b[0;36m_BaseOptimizer.minimize\u001b[1;34m(self, loss, var_list, tape)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mminimize\u001b[39m(\u001b[39mself\u001b[39m, loss, var_list, tape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    522\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Minimize `loss` by updating `var_list`.\u001b[39;00m\n\u001b[0;32m    523\u001b[0m \n\u001b[0;32m    524\u001b[0m \u001b[39m    This method simply computes gradient using `tf.GradientTape` and calls\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[39m      None\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 542\u001b[0m     grads_and_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_gradients(loss, var_list, tape)\n\u001b[0;32m    543\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_gradients(grads_and_vars)\n",
      "File \u001b[1;32mc:\\Users\\oyjuf\\Documents\\GitHub\\DRL-A1\\.venv\\lib\\site-packages\\keras\\optimizers\\optimizer.py:261\u001b[0m, in \u001b[0;36m_BaseOptimizer.compute_gradients\u001b[1;34m(self, loss, var_list, tape)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compute gradients of loss on trainable variables.\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \n\u001b[0;32m    245\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39m  gradient can be `None`.\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mcallable\u001b[39m(loss) \u001b[39mand\u001b[39;00m tape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 261\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    262\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`tape` is required when a `Tensor` loss is passed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    263\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReceived: loss=\u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m}\u001b[39;00m\u001b[39m, tape=\u001b[39m\u001b[39m{\u001b[39;00mtape\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    264\u001b[0m     )\n\u001b[0;32m    265\u001b[0m \u001b[39mif\u001b[39;00m tape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    266\u001b[0m     tape \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mGradientTape()\n",
      "\u001b[1;31mValueError\u001b[0m: `tape` is required when a `Tensor` loss is passed. Received: loss=4.041056156158447, tape=None."
     ]
    }
   ],
   "source": [
    "run_environment()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
