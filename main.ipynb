{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - CATCH\n",
    "TODO:\n",
    "- Experience Replay Buffer - Done\n",
    "- Mini Batch Training\n",
    "- A Target Network\n",
    "- Reward Clipping\n",
    "- Loss Function Implementation\n",
    "- CNN\n",
    "\n",
    "Sources: \\\n",
    "https://github.com/yxu1168/Reinforcement-Learning-DQN-for-ATARI-s-Pong-Game---TensorFlow-2.0-Keras/blob/master/Reinforcement_Learning_CNN_DQN_for_ATARI%E2%80%99s_Pong_Game.ipynb\n",
    "\n",
    "https://github.com/PacktPublishing/Hands-on-Reinforcement-Learning-with-PyTorch/blob/master/Section%203/3.5%20DQN%20with%20Pong.ipynb\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatchEnv():\n",
    "    def __init__(self):\n",
    "        self.size = 21\n",
    "        self.image = np.zeros((self.size, self.size))\n",
    "        self.state = []\n",
    "        self.fps = 4\n",
    "        self.output_shape = (84, 84)\n",
    "\n",
    "    def reset_random(self):\n",
    "        self.image.fill(0)\n",
    "        self.pos = np.random.randint(2, self.size-2)\n",
    "        self.vx = np.random.randint(5) - 2\n",
    "        self.vy = 1\n",
    "        self.ballx, self.bally = np.random.randint(self.size), 4\n",
    "        self.image[self.bally, self.ballx] = 1\n",
    "        self.image[-5, self.pos - 2:self.pos + 3] = np.ones(5)\n",
    "\n",
    "        return self.step(2)[0]\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        def left():\n",
    "            if self.pos > 3:\n",
    "                self.pos -= 2\n",
    "        def right():\n",
    "            if self.pos < 17:\n",
    "                self.pos += 2\n",
    "        def noop():\n",
    "            pass\n",
    "        {0: left, 1: right, 2: noop}[action]()\n",
    "\n",
    "\n",
    "        self.image[self.bally, self.ballx] = 0\n",
    "        self.ballx += self.vx\n",
    "        self.bally += self.vy\n",
    "        if self.ballx > self.size - 1:\n",
    "            self.ballx -= 2 * (self.ballx - (self.size-1))\n",
    "            self.vx *= -1\n",
    "        elif self.ballx < 0:\n",
    "            self.ballx += 2 * (0 - self.ballx)\n",
    "            self.vx *= -1\n",
    "        self.image[self.bally, self.ballx] = 1\n",
    "\n",
    "        self.image[-5].fill(0)\n",
    "        self.image[-5, self.pos-2:self.pos+3] = np.ones(5)\n",
    "    \n",
    "        terminal = self.bally == self.size - 1 - 4\n",
    "        reward = int(self.pos - 2 <= self.ballx <= self.pos + 2) if terminal else 0\n",
    "\n",
    "        [self.state.append(resize(self.image, (84, 84))) for _ in range(self.fps - len(self.state) + 1)]\n",
    "        self.state = self.state[-self.fps:]\n",
    "\n",
    "        return np.transpose(self.state, [1, 2, 0]), reward, terminal\n",
    "\n",
    "    def get_num_actions(self):\n",
    "        return 3\n",
    "\n",
    "    def reset(self):\n",
    "        return self.reset_random()\n",
    "\n",
    "    def state_shape(self):\n",
    "        return (self.fps,) + self.output_shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size = 1e6):\n",
    "        self.buffer = []\n",
    "        self.max_size = max_size\n",
    "        self.overFlow = 0\n",
    "    \n",
    "    def add(self, data):\n",
    "        if len(self.buffer) == self.max_size:\n",
    "            self.buffer[self.overFlow] = data\n",
    "            self.overFlow = (self.overFlow + 1) % self.max_size\n",
    "        else:\n",
    "            self.buffer.append(data)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.randint(0, len(self.buffer), size=batch_size)\n",
    "\n",
    "        trajectories = []\n",
    "\n",
    "        for i in indices:\n",
    "            trajectories.append(self.buffer[i])\n",
    "\n",
    "        return trajectories"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Action Value Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QFunction:\n",
    "    pass\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 20, 20, 16)        3088      \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 9, 9, 32)          8224      \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 7, 7, 32)          9248      \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1568)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                100416    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 125,331\n",
      "Trainable params: 125,331\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = [84, 84, 3] \n",
    "action_size = 3\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(16, kernel_size=8, strides=4, input_shape = input_shape, activation=\"relu\"),\n",
    "    keras.layers.Conv2D(32, kernel_size=4, strides=2, activation=\"relu\"),\n",
    "    keras.layers.Conv2D(32, kernel_size=3, strides=1, activation=\"relu\"), \n",
    "    keras.layers.Flatten(),                            \n",
    "    keras.layers.Dense(64, activation=\"relu\"),\n",
    "    keras.layers.Dense(64, activation=\"relu\"),\n",
    "    keras.layers.Dense(action_size)\n",
    "])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent():\n",
    "    def __init__(self):\n",
    "        self.action_size = 3\n",
    "        self.train = model\n",
    "        self.target = model\n",
    "        self.epsolon = 0.1\n",
    "\n",
    "    # select an action based on epsilon greedy method\n",
    "    def action():\n",
    "        \n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randint(3)\n",
    "        else:\n",
    "            return 2\n",
    "\n",
    "    # train the model\n",
    "    def train():\n",
    "        pass\n",
    "\n",
    "    # update the target network\n",
    "    def update():\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_environment():\n",
    "    env = CatchEnv()\n",
    "    buffer = ReplayBuffer()\n",
    "    number_of_episodes = 1\n",
    "\n",
    "    for ep in range(number_of_episodes):\n",
    "        env.reset()\n",
    "        \n",
    "        state, reward, terminal = env.step(random.randint(0,2))\n",
    "\n",
    "        while not terminal:\n",
    "            state, reward, terminal = env.step(random.randint(0,2))\n",
    "\n",
    "            print(\"Reward obtained by the agent: {}\".format(reward))\n",
    "            state = np.squeeze(state)\n",
    "\n",
    "\n",
    "            # Add data to replay buffer\n",
    "            action = 1\n",
    "            stateNext = 0\n",
    "\n",
    "            buffer.add([state, action, reward, stateNext])\n",
    "\n",
    "            trajectories = buffer.sample(4)\n",
    "\n",
    "\n",
    "            # plt.matshow(state)\n",
    "            # plt.show()\n",
    "\n",
    "        print(\"End of the episode\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward obtained by the agent: 0\n",
      "Reward obtained by the agent: 0\n",
      "Reward obtained by the agent: 0\n",
      "Reward obtained by the agent: 0\n",
      "Reward obtained by the agent: 0\n",
      "Reward obtained by the agent: 0\n",
      "Reward obtained by the agent: 0\n",
      "Reward obtained by the agent: 0\n",
      "Reward obtained by the agent: 0\n",
      "Reward obtained by the agent: 0\n",
      "End of the episode\n"
     ]
    }
   ],
   "source": [
    "run_environment()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
